\documentclass{csfourzero}
\usepackage{url, natbib, upquote, multicol}

\title{The effect of corruption on search engine query prediction}
\author{Charlie F.\ Egan}
\date{\today}

\bibliographystyle{plain}
\abstract{Search engines have are able to learn from user behavoir to improve their results. Rather than relying on a traditional spelling correction algorithms, user query patterns can be used to train the system and provide more accurate predictions. This report investigates how increasingly corrupted input effects the prediction accuracy of the system and well as how each in a number of top search engines compare in their ability to predict intended input. Related work and concepts are also discussed. It was found that search engines differ significantly in their prediction accuracy and that there is no relation between the word length performance.}

\begin{document}
\maketitle

\section{Introduction}
\label{sec:intro}

Search engines offering query predictions are the research interest of the project. Search engines offer predictions to overcome corruption in user queries, where corruption is the combined effect of one or more typographical errors. While prediction performance is high, the details of processes behind predictions are not always fully disclosed. Failed user queries followed by a user's own corrections can build an aggregated list of corrupted inputs for an intended result, for longer queries the probabilities for words in context can also be accounted for \cite{noampatent}. Probabilistic edit-distance implementations are also used \cite{howtospellcorrector}.

The project aim is to evaluate the variation in the prediction accuracy in responses, for increasingly corrupted terms, queried on a number of common search engines. Prediction accuracy is capability of a search engine suggest or adopt the intended term, given a corrupted version. The effect of query length in characters, for a given rate of corruption, is also investigated as part of the project.

Information about search engine prediction accuracy for corrupted queries would be useful for those making mechanical use of search engines for spelling correction. A ranked list of search engines by prediction accuracy could also be of interest to those with poor spelling or dexterity from conditions such as Dyslexia or Parkinson's disease. Also of interest would be the relationship between market share and the performance of query prediction as a feature.

Sections 2-7 cover: spelling correction background as a task in natural language processing, the research questions, the design of the comparison experiments, the results, their discussion and conclusion respectively.

\section{Background and Related Work}
\label{sec:lit}

``Correction is the task of substituting the well-spelled hypotheses for misspellings." \cite{webuser4google2009}

Computerized spelling correction has been undergoing research since 1957 \cite{jameslpeterson1980}. Early approaches offered predications for strings with typographical errors by calculating edit distance \cite{1992correctiondiscussion}. The edit distance of two strings is the number of edit operations required to transform one string into another \cite{introIR}. In 1966 Levenshtein described a model for such transformations \cite{levenshtein1966binary}. \textit{Levenshtein distance} has been the basis for many error models used in spelling correction implementations since.

However, calculating edit distances for a string against large dictionaries is computationally expensive \cite{2009adaptivespellchecker}. A number of more recent implementations have been built using a \textit{Noisy Channel Model}, based on Shannon's \textit{Noisy Channel Theorem}.

In 1990 once such program, \textit{correct}, was implemented at \textit{Bell Laboratories} to offer correction from non-word errors detected by the Unix \textit{spell} program \cite{originalnoisychannel}. The tool generated all candidate corrections for a misspelling by applying a single deletion, insertion, transposition or substitution at each position. These candidate corrections are ranked by a combination of their frequency in a large corpus and the probability of the misspelling given the correction. The later probability, and the model of the noisy channel, is calculated using 'confusion matrices' that store the relative frequencies of different single letter corruptions for pairs of letters. This error model was extended by Brill Moore in 2000 \cite{betternoisychannel} to represent 'string to string' edits for words with multiple errors. This improvement led to a 52\% reduction in spelling correction errors.

Search engine query correction poses new challenges. 10-15\% of search engine queries contain errors, and short queries make contextual approaches used in word processing impractical \cite{webuserpoweredspelling}. Corrections for non-word, typographical errors cannot be not fully accounted for with dictionaries \cite{webuser3}, for example, \textit{verizon} may be the intended term but would likely be corrected to \textit{version} or \textit{horizon} with a traditional approach. Real word (cognitive) errors, such as \textit{'an introduction to \textbf{sea} programming'} are also hard to model in dictionary implementations.

Corruptions in query terms is comparable to the study of single nucleotide polymorphisms and copy-number variations in Genetics.

\section{Research question}
\label{sec:rq}

Here is clearly much work being done in the space of spelling correction automation, implementations used by search engine companies are closed source and little is known of the specifics CHECK. One aim of this project is to investigate these systems to see what can be inferred about their implementation and whether or not they differ. The research questions for the project are as follows:

\begin{itemize}
  \item{What is the effect of query length on the suggestion accuracy for a given rate of corruption?}
  \item{Is there a significant difference in prediction accuracy between search engines?}
\end{itemize}

To address these questions the prediction accuracy for a number of search engines will be compared when queried with corrupted terms of various lengths. Each search engine will be tested with the same set of corrupted terms and the accuracy of their returned predictions recorded.

The following search engines will be tested: \textit{Ask, Baidu, Bing, DuckDuckGo, Google, Sogou, Yahoo, Yandex and Youdao}. These non-aggregating search engines all offer suggestions for misspelled queries and represent a majority of global general search engines \cite{searchenginewiki}. A parallel scraper \cite{scraper} has been implemented to collect the results of each for a given query. Five instances of this scraper will be deployed to disposable environments on the Heroku PaaS and requests made against each to gather results from a local script.

The terms selected to derive corruptions will be .com website names of a given length selected from the \textit{Alexa Top 500} \cite{alexatop500}, these seed terms are listed in Appendix 1. Brand names that are either four, eight or twelve characters in length were selected to allow a constant corruption rate of 25\% to be applied. Longer word seeds were few and were excluded to ensure predominantly single word brands names were used.

Corruptions will be generated from seed terms using a substitution algorithm, see Appendix 2. A corruption is applied by selecting a random, uncorrupted character and substituting it with an adjacent key on the keyboard to simulate typographical errors. Transformations, as based on the US (UK Macintosh) keyboard, are listed in Appendix 3. Insertions of additional letters, letter deletions and transpositions will not be included to ensure consistency.

In related work, spelling correction system performance is commonly assessed using Accuracy, alongside both Precision and Recall. In this experiment where, all queries are of single class, corrupted at 25\%, search engines will be compared on their \textit{Accuracy}. That is, for a given query set, what percentage of of corrupted queries cause the search engine to return the seed term. Given their rarity, collisions of corrupted spellings with other uncorrupted queries assumed to be insignificant.

\section{Experimental Design}
\label{sec:exp}

The null hypotheses for the experiment are as follows:
\begin{itemize}
  \item{At a constant rate of corruption, accuracy is not significantly affected by the query length in characters. (\textbf{H1})}
  \item{Search engines do not differ significantly in their average accuracy for a range of query lengths. (\textbf{H2})}
\end{itemize}

The target population is all search terms that are the result of one or more typographical corruptions. Due to search engine rate-limiting and time constraints it is infeasible to test these in large numbers. Random seed terms such as strings of words would not be representative of real world queries. A consistent collection of realistic query phrases is challenging to generate without introducing bias. In this experiment Alexa ranked word (or compound word) terms 4, 8 and 12 characters in length were selected. These are highly representative of typical queries and can be consistently corrupted at 25\%, such restrictions help reduce any bias. There were too few 16 character terms for them to be included as an additional set of seed terms.

At each query length (4, 8, 12), 100 terms were generated from a number of seed terms with 25\% corruption (Appendix 2). Seed terms used for each length can be found in Appendix 3. Accuracy is defined as the percentage of seed terms returned for their corresponding corrupted term. The search engine accuracy and term length are the independent and dependent variables respectively.

To test the hypotheses, the search engine responses for the each of the 100 corrupted terms for each length will be recorded. The accuracies of each search engine at each word length will be compared. Should the differences in average accuracy be found to be significant (p \textless 0.05) between length then there will be reason to reject the null hypothesis \textbf{H1}.

If the search engine accuracy difference is found to be significant (p \textless 0.05) then this will be cause to reject \textbf{H2}.

\section{Results}
\label{sec:results}

Present the results. A good way to organise this is via subsections
for each hypothesis you tested. Include graphs of results
, tests of significance, etc. If you have
negative results, include them. A negative results is just as
informative and useful as a positive one, sometimes more so.

Guide length: 500 words.

table of the results in summary, same sentence ref a graph of the table.

the results suggest that all decreased in accuracy as the number of corruptions increased. Also x did better then y - quote values to support / highlight things in the table

then do a section for each comparison and hypothesis / quesion 5.1 5.2 - say was was and was not significant, in response to the question. Quote those p values!

Segment the section based on the number of questions, accuracy vs speed etc. for each quote the values of the statistical tests.

quote the p values and the difference in the results from the tukey HSD

\section{Discussion}
\label{sec:discuss}

What do the results say? What have you learned from the
experiments? Have you identified a correlation between variables, or
causation? What are the limitations of what you've done? What further
experiments might be of benefit?

Guide length: 400 words.

results summary, state the relationships at a high level, make references to the values of the tests mean stdev etc.

limitations of the study. limited in data and tools compared. talk about any bias, are my typos real for instance? likely not.

How to do it better next time, make better representation of the population, big pg on what was missed and how this might be an issue.

Keep it general - what does the data tell us about search engines in general?

\section{Conclusion}
\label{sec:conc}

What have you done and why? What have you shown through your
experiments?

Guide length: 100 words.

tiny, 1 pg on the work done, why done "id the best" / "test the difference", results summary, best worst wrt hyps,

I have conducted an experiment to test...

What are the implications of the results? What do they mean at a higher level?

\raggedbottom
\bibliography{myrefs}

\pagebreak
\raggedbottom
\section{Appendix 1: Seed Terms}
\textbf{Four Letter Seeds}
\textit{\{Ebay, Bing, Imdb, Etsy, Yelp, Cnet, Vice, Ikea, 9gag, Hulu, Dell, Citi, Asos, Java\}}

\noindent
\textbf{Eight Letter Seeds}
\textit{\{Linkedin, Blogspot, Flipkart, Outbrain, Buzzfeed, Whatsapp, Softonic, Usatoday, Mashable, Engadget, Gsmarena, Evernote, Theverge\}}

\noindent
\textbf{Twelve Letter Seeds}
\textit{{Spaceshipads, Secureserver, Shutterstock, Espncricinfo, Steampowered, Mercadolivre, Extratorrent, Liveinternet, Infusionsoft, Surveymonkey}}

\pagebreak
\raggedbottom
\section{Appendix 2: Corruption Algorithm}
  \begin{verbatim}
    define corrupt(string, count, substitution_map)
      string = string to downcase
      indexes = range(1 to string.length) in random order

      loop count times
        index_to_corrupt = indexes.pop
        character_to_corrupt = string[index_to_corrupt]
        possible_replacements = substitution_map[character_to_corrupt]
        replacement_character = random element from possible_replacements
        string[index_to_corrupt] = replacement_character
      end

      return string
    end
  \end{verbatim}

\pagebreak
\section{Appendix 3: Letter Substitutions}
\begin{multicols}{2}
  \begin{verbatim}
  a : {q w s z `}
  b : {v g h n}
  c : {x d f v}
  d : {s e r f c x}
  e : {w 3 4 r d s}
  f : {d r t g v c}
  g : {f t y h b v}
  h : {g y u j n b}
  i : {u 8 9 o k j}
  j : {h u i k m n}
  k : {j i o l , m}
  l : {k o p ; . ,}
  m : {n j k ,}
  n : {b h j m}
  o : {i 9 0 p l k}
  p : {o 0 - [ ; l}
  q : {1 2 w a}
  r : {e 4 5 t f d}
  s : {a w e d x z}
  t : {r 5 6 y g f}
  u : {y 7 8 i j h}
  v : {c f g b}
  w : {q 2 3 e s a}
  x : {z s d c}
  y : {t 6 7 u h g}
  z : {` a s x}
  . : {, l ; /}
  ! : {@ 2 1 q §}
  0 : {- p o 9}
  1 : {§ q 2}
  2 : {1 q w 3}
  3 : {2 w e 4}
  4 : {3 e r 5}
  5 : {4 r t 6}
  6 : {5 t y 7}
  7 : {6 y u 8}
  8 : {7 u i 9}
  9 : {8 i o 0}
  - : {0 p [ =}
  \end{verbatim}
\end{multicols}


\end{document}
