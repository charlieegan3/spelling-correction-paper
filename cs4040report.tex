\documentclass{csfourzero}
\usepackage{url, natbib, upquote, multicol, caption, subcaption, float, algorithmicx, algpseudocode}
\setlength{\bibsep}{0pt plus 0.3ex}

\title{The effect of corruption on search engine query correction}
\author{Charlie F.\ Egan}
\date{\today}

\bibliographystyle{plain}
\abstract{Search engines are able to learn error correction from the behavior of their users. Rather than relying on a traditional spelling correction algorithms, user query patterns can be used to train tools and provide more accurate corrections as a result. This report investigates how corrupted input effects the correction accuracy of the system and well as how each in a number of top search engines compare in their ability to correct corrupted input - related work and concepts are also discussed. It was found that search engines differ significantly in their correction accuracy and that there is a relation between word length and correction performance.}

\begin{document}
\maketitle

\section{Introduction}
\label{sec:intro}

Search engines offering query corrections are the research interest of the project. Search engines offer corrections to overcome corruption in user queries. Corruption is defined as the combined effect of one or more typographical errors. While correction performance is good, details of tools used to compute corrections are not commonly disclosed. Failed user queries followed by a user's own corrections can build an aggregated list of corrupted inputs for an intended result, for longer queries the probabilities for words in context can also be accounted for \cite{noampatent}. Probabilistic edit-distance implementations are also used \cite{howtospellcorrector}.

The project aim is to evaluate the variation in the correction accuracy of responses from a number of common search engines. Correction accuracy is defined as the capability of a search engine suggest or adopt the intended query, given a corrupted version. This is tested using queries of different lengths for a given rate of corruption.

Information about search engine correction accuracy, would be useful for those making mechanical use of search engines. A ranked list of search engines by correction accuracy could also be of interest to those with poor spelling or dexterity from conditions such as Dyslexia or Parkinson's disease. More generally, the effect of length in corrupted strings on error correction for information transfer using an error prone channel is of broader interest.

Sections 2-7 cover: spelling correction background as a task in natural language processing, the research questions, the design of the comparison experiments, the results, their discussion and conclusion respectively.

\section{Background and Related Work}
\label{sec:lit}

``Correction is the task of substituting the well-spelled hypotheses for misspellings." \cite{webuser4google2009}

Computerized spelling correction has been of research interest since 1957 \cite{jameslpeterson1980beginning}. Early approaches offered corrections for strings with typographical errors by calculating edit distance \cite{1992correctiondiscussion}. The edit distance of two strings is the number of edit operations required to transform one string into another \cite{introIR}. In 1966 Levenshtein described a model for such transformations \cite{levenshtein1966binary}. \textit{Levenshtein distance} has been the basis for many error models used in spelling correction implementations since.

However, calculating edit distances for a string against large dictionaries is computationally expensive \cite{2009adaptivespellchecker}. A number of more recent implementations have used a \textit{Noisy Channel Model}, based on Shannon's \textit{Noisy Channel Theorem}. One such program, \textit{correct}, detailed in a paper by Kernighan et al. (1990), offered correction from non-word errors detected by the Unix \textit{spell} program \cite{originalnoisychannel}. The tool generated candidate corrections for a misspelling by applying a single deletion, insertion, transposition or substitution at each position. These candidate corrections are ranked by a combination of their frequency in a larger corpus and the probability of the misspelling given the correction. This probability, and model of the noisy channel, is calculated using \textit{confusion matrices} that store the relative frequencies of different single letter corruptions for pairs of letters. This error model was later extended by Brill \& Moore (2000) \cite{betternoisychannel} to represent \textit{string to string} edits for words with multiple errors. Accounting for multiple errors led to a 52\% reduction in the spelling correction tool error rate.

Corruptions of search queries over a noisy channel is comparable to the study of single nucleotide polymorphisms and copy-number variations in Genetics, deep space telecommunications and wireless video streaming. Error correction is a process beyond spelling.

Search engine query correction poses new challenges. 10-15\% of search engine queries contain errors, and short queries make contextual approaches used in word processing of larger documents impractical \cite{webuserpoweredspelling}. Corrections for non-word, typographical errors cannot be not fully accounted for using dictionaries \cite{webuser3}, for example, \textit{verizon} may be the intended query but would likely be corrected to \textit{version} or \textit{horizon} by a traditional implementation. Real word or cognitive errors, such as \textit{'an introduction to \textbf{sea} programming'}, are also hard to model in dictionary implementations.

The first documented approach to make use of user search patterns was that of Brill \& Cucerzan in 2004 \cite{webuserpoweredspelling}. Their implementation compared search engine query logs with a large corpus to gather candidate corrections and used a context-dependent, weighted, edit distance error model was used to make comparisons. Given that the majority of queries are correct, the transformations can be iteratively applied to arrive at more common (correct) queries. The results of the system, the first published approached to utilize query logs, aligned 82\% of the time with human annotators with high precision and recall. This approach was improved upon with the inclusion of additional metrics such as page count (for a given query) by Chen et al. (2007) \cite{webuser3}.

A similar approach employed an \textit{Expectation Maximization} algorithm instead of a of an annotated corpus of corrections \cite{webuser2learningerrormodel}, while comparable, it did not perform as well as implementations that relied on manually derived information. A corpus (and language) independent implementation first appeared in 2009, Whitelaw et al. \cite{webuser4google2009}. This was the first system to remove the hand-labeled data requirement. Information about misspellings are inferred from their use in query logs, common queries are used as a list of candidate corrections. The system is built fundamentally on the Noisy Channel Model \cite{claudeshannon1948}.

Despite top search companies contributing much to the area \cite{webuser3, webuserpoweredspelling, microranker, microphone, webuser4google2009}, comparable information about their implementations is not available. Additionally, the general effect of data length on error correction rates over a error prone channel does not appear to have been made.

\section{Research question}
\label{sec:rq}

While automated spelling correction is an active topic of research, little is known of the implementations used by search engine companies to calculate corrections. An aim of this project is to superficially investigate these systems and learn whether or not their implementations differ. However, the effect of query length is the experimental focus. The research questions for the project are as follows:

\begin{itemize}
  \item{What is the effect of query length on the correction accuracy for a given error rate?}
  \item{Is there a significant difference in correction accuracy between search engines?}
\end{itemize}

\noindent
To address these questions the correction accuracy for a number of search engines will be compared. Search engines will be queried with corrupted queries of various lengths. Each search engine will be tested with the same set of queries and the accuracy of their returned spelling corrections recorded.

The following search engines will be tested: \textit{Ask, Baidu, Bing, DuckDuckGo, Google, Sogou, Yahoo, Yandex} and \textit{Youdao}. These non-aggregating search engines all offer corrections for misspelled queries and represent the majority of global general search engines \cite{searchenginewiki}. A parallel web scraper has been implemented to programmatically run queries and parse results of each search engine for a given query \cite{scraper}. Five instances of this scraper will be deployed to disposable environments on the Heroku platform. Requests are then made against each instance to gather results using a local script to aggregate results for each sample.

Due to search engine rate-limiting and time constraints it is infeasible to test corrupted queries in large numbers and randomly generated seed queries such as strings of unrelated words would not be representative of real world queries. This makes the generation of a consistent collection of realistic query phrases is challenging without introducing bias. In this experiment, seed terms were selected from the Alexa Top 500 \cite{alexatop500}. The work is built on the assumption that these are a good representation of real user queries. From the 500 domains, only .com variations were used to ensure that each brand was only used once, and not for each top-level domain the brand operated. These .com brands make up 63\% of the top 500. Seed queries to be used for generation of the corrupted samples are listed in Appendix 1.

Corruptions will be generated from seed queries using a substitution algorithm, see Appendix 2, this and the possible substitutions for each character represent the error model of the study. A corruption is applied by selecting a random, unchanged character in the original string and making a substitution with an adjacent key on the keyboard to simulate typographical (non-word) errors. Transformations are based on the US (UK Macintosh) keyboard and are listed in Appendix 3. Corruptions may be applied a number of times to a seed, however the same index cannot be corrupted more than once. This ensure that all corruptions are adjacent keys and that the edit distance is constant. Insertions of additional letters, letter deletions and transpositions are not included as part of this study. In a suitably large sample sample collisions of corrupted queries and other valid queries is assumed to be insignificant.

\section{Experimental Design}
\label{sec:exp}

There to answer each question, two null hypotheses were defined for the study. These are as follows:
\begin{itemize}
  \item{At a constant rate of corruption, accuracy is not significantly affected by the query length in characters. (\textbf{H1})}
  \item{Search engines do not differ significantly in the accuracy in correction of corrupted queries. (\textbf{H2})}
\end{itemize}

\noindent
The target population for the study is all probable user search queries that are the result of one or more typographical errors. Elements in each sample length are based on a seed query, this values retained and used to test the corrections returned. Seed queries are selected from website brands of a given length, these are then randomly corrupted at a fixed rate to generate samples. Brands that are either four, eight or twelve characters in length were selected to allow a constant corruption rate of 25\% to be applied. There were no qualifying 16 letter terms. Correction rates for $\frac{1}{3}$ were also too low making 3,6,9,12,15 an unsuitable set of sample lengths. Seed queries are listed in Appendix 1. There are a greater number of four letter brands, more than 3 times as many twelve letter terms, though they are distributed among the top domains this source of potential bias was unfortunately unavoidable. All terms on the Alexa list are assumed to have significant search volume at all search engines under test.

At each query length (four, eight, twelve characters), 100 queries were generated from the seeds (Appendix 1) with a constant 25\% corruption rate using the adjacent substitution algorithm in Appendix 2. Substations are applied once, twice and three times for four, eight and twelve character queries respectively - each index is only ever substituted once. For a given sample of corrupted queries, search engine accuracy is defined as: the percentage of queries in the sample for which the search engine returns the seed of the corrupted query as a correction on the results page. On this page, corrections are either present or missing, it is assumed that, since queries are single words, partial corrections are not possible. The search engine accuracy is the independent variable in all comparisons. Length and search engine are the dependent variables for the first and second questions respectively.

To test the hypotheses, the search engine responses for the each of the 100 corrupted queries for each of the three query lengths will be recorded. The combined accuracy of all search engines at each length will be tested for a group effect. Should the differences in average accuracy be found to be significant, at the 95\% confident interval $(p < 0.05)$, between length then there will be reason to reject the first null hypothesis \textbf{H1}.

To address the second question, using the same set of results, an average score for each search engine will be calculated using the results for all lengths. If the difference between these averages is found to be significant, again, at the 95\% confidence interval $(p < 0.05)$, then this will be cause to reject the second hypothesis \textbf{H2}.

\section{Results}
\label{sec:results}

\begin{figure}
  \centering
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{len_vs_acc}
    \captionof{figure}{Mean search engine accuracy for each query length}
    \label{fig:lengths}
  \end{minipage}%
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{eng_vs_acc}
    \captionof{figure}{Mean accuracies over all query lengths}
    \label{fig:searchengines}
  \end{minipage}
\end{figure}

Figure~\ref{fig:lengths} shows the average for all nine search engines for each query length. Figure~\ref{fig:searchengines} shows the average accuracy over all query length for each search engine. Error bars in both figures represent standard errors in the mean values.

The results suggest that correction accuracy decreases as the query length increases. The effect is most apparent for twelve character queries. While queries four and eight characters in length have more similar accuracy values there still appears to be a difference. The results also show a clear difference in search engine correction accuracy performance. Surprisingly, DuckDuckGo returned the most accurate corrections, performing 11\% above average and 23\% above Yandex, the poorest performing engine. DuckDuckGo is the search engine with the second lowest Alexa rank of those under test. Also of note was the difference between four and eight characters for Google - it was the only search engine to perform better on eight character queries.

\subsection{Term length comparison}
Average correction accuracy was calculated using 900 query results from from all search engines. This was calculated for each of the 3 lengths and plotted in Figure~\ref{fig:lengths}, errors shown are the standard error in the mean. Using these results, an f-test was performed to make a comparison with the first null hypothesis \textbf{H1}, that length has no effect on the correction accuracy. The result, $\big((F_{2,24}) = 3.53, p = 0.045\big)$, allows this null hypothesis to be rejected at the 95\% confidence interval as the difference between lengths was shown to be significant.

Given that query length has a significant effect on corrector accuracy a Tukey's HSD test was also carried out to closer investigate differences in the means of the 3 lengths. Only the four vs twelve character comparison gave a result within the 95\% confidence interval $\big(p = 0.038\big)$. Comparisons between adjacent groups were not found to be significant on the data gathered in this project. Both four-eight, and eight-twelve pairwise comparisons resulted in values for $p > 0.2$, $0.64$ and $0.22$ respectively. A larger sample would further refine these values and could reveal a significant difference, however given the low scores it would need to be a large increase. The trend was that shorter queries had a better rate of correction with the biggest difference being between the samples for eight and twelve character queries.

\subsection{Search engine comparison}
Using the same data collected for the query length question it was also possible to also make a comparison between each search engine. All results for each query length were averaged and plotted with standard errors in the mean in Figure~\ref{fig:searchengines}. An f-test was then performed to examine differences between each individual search engine, across all three lengths. This gave a strong result $\big((F_{8,18}) = 4.576, p = 0.0035\big)$ and justifies rejection of the second null hypothesis \textbf{H2}, that all search engines are equally capable of correction for corrupted user queries.

DuckDuckGo returned the most accurate corrections overall. Bing and Google where close on four and eight characters respectively with eight characters interestingly being the best Google result. Yandex had the poorest correction rate and failed to correct any queries in the twelve character sample, Ask also only returned a single correction for this sample. Please see Appendix 4 for a combined plot comparing the 9 search engine correction accuracy rates at each length.

\section{Discussion}
\label{sec:discuss}

The results appear to suggest that length does impact correction accuracy, when other variables remain constant. This pattern was consistent across all but one search engine. This would suggest that spelling correction implementations are better suited to shorter query queries. Also, more generally, that error correction of corrupted queries from an error prone channel is more successful for shorter strings.

A secondary finding, that search engines differ in their correction accuracy is also of interest. This shows that each are likely using substantially different implementations. It is also of interest that greater query volume is not required to for best-in-class performance. Google performed worse than DuckDuckGo and handles more than 20 times the number of queries. This would suggest that, despite an apparent trend towards using user activity to train error models \cite{webuserpoweredspelling, webuser3, webuser2learningerrormodel, webuser4google2009}, it is not the only requirement for a high performing, error correction implementation. It is also possible that a critical point exists where more behavioral data yields little benefit.

The trend in is perhaps partially explained by the number of Alexa brands at each length. four, eight, and twelve characters have counts 35, 28, 9 respectively. However, this does not explain the difference in the Google trend. It is possible that the eight letter seed terms are more common queries on Google than other search engines - though this seems unlikely.

The most apparent bias in the study is introduced by the inclusion of both Asian and US search engines. This was done to give sufficient data for each query length - including additional query lengths would have introduced further bias (inconsistent error rate or word count). Seed queries did include Asian but the ratio is not comparable to 5:4 US Asia split for search engines.

Error generation was also strictly controlled to minimise bias in the length comparison. The algorithm in Appendix 2 is a compromise and is intended to represent realistic typographical errors. Real error models are more representative - but in turn less controlled. Real word errors as well as insertions, deletions and transposition are not covered in this study. A similar comparison based on error types and their positions within a string would be interesting.

Brand names were selected seed queries as they are likely to be consistently well ranked. However, they do not allow for a wide range in query lengths without introducing potential bias from word count differences. A repeat study that used a wider range of query lengths would be make for a good comparison. Wikipedia article titles were considered for this study.

The experiment design was focused on the first question. A more detailed, search engine centric, comparison would include a wider range of query types and errors as detailed above. This study was based on ``Did you mean" prompts, another more realistic metric might be checking the relevance of the first result for a given corrupted query.

\section{Conclusion}
\label{sec:conc}

This study investigated the effect of query length on error correction accuracy by testing responses for queries with non word errors from a number of search engines. This was done to better understand error correction, and more specifically, the effect of query length. The results suggest that increasing length, for a constant rate of corruption, as a negative impact on correction accuracy. They also show a clear, and in cases unexpected, difference in the accuracy between search engines. This shows that for the search engine domain, error correction remains an unsolved problem.

\raggedbottom
\pagebreak
\bibliography{myrefs}

\pagebreak
\raggedbottom
\section{Appendix 1: Seed Terms}
\textbf{Four Letter Seeds}
\textit{\{Ebay, Bing, Imdb, Etsy, Yelp, Cnet, Vice, Ikea, 9gag, Hulu, Dell, Citi, Asos, Java\}}

\noindent
\textbf{Eight Letter Seeds}
\textit{\{Linkedin, Blogspot, Flipkart, Outbrain, Buzzfeed, Whatsapp, Softonic, Usatoday, Mashable, Engadget, Gsmarena, Evernote, Theverge\}}

\noindent
\textbf{Twelve Letter Seeds}
\textit{{Spaceshipads, Secureserver, Shutterstock, Espncricinfo, Steampowered, Mercadolivre, Extratorrent, Liveinternet, Infusionsoft, Surveymonkey}}

\pagebreak
\raggedbottom
\section{Appendix 2: Corruption Algorithm}
  \begin{algorithmic}
    \Function{corruptString}{$string$, $count$}
        \State $string \gets downcase(string)$
        \State $indexes \gets$ $\{ x \in \mathbb{Z} \ \vert \ 1 \leq x \leq length(string) \}$
        \State $indexes \gets$ $randomOrder(indexes)$
        \Repeat
        \State $indexToCorrupt \gets pop(indexes)$
        \State $characterToCorrupt \gets string[indexToCorrupt]$
        \State $replacementCharacters \gets keyboardSubstitutionsForCharacter(characterToCorrupt)$
        \State $string[indexToCorrupt] \gets randomElement(replacementCharacters)$
        \State $count \gets count - 1$
        \Until{count is 0}
        \State \Return $string$
    \EndFunction
  \end{algorithmic}

\pagebreak
\raggedbottom
\section{Appendix 3: Letter Substitutions}
\begin{multicols}{2}
  \begin{verbatim}
  a : {q w s z `}
  b : {v g h n}
  c : {x d f v}
  d : {s e r f c x}
  e : {w 3 4 r d s}
  f : {d r t g v c}
  g : {f t y h b v}
  h : {g y u j n b}
  i : {u 8 9 o k j}
  j : {h u i k m n}
  k : {j i o l , m}
  l : {k o p ; . ,}
  m : {n j k ,}
  n : {b h j m}
  o : {i 9 0 p l k}
  p : {o 0 - [ ; l}
  q : {1 2 w a}
  r : {e 4 5 t f d}
  s : {a w e d x z}
  t : {r 5 6 y g f}
  u : {y 7 8 i j h}
  v : {c f g b}
  w : {q 2 3 e s a}
  x : {z s d c}
  y : {t 6 7 u h g}
  z : {` a s x}
  . : {, l ; /}
  ! : {@ 2 1 q ยง}
  0 : {- p o 9}
  1 : {ยง q 2}
  2 : {1 q w 3}
  3 : {2 w e 4}
  4 : {3 e r 5}
  5 : {4 r t 6}
  6 : {5 t y 7}
  7 : {6 y u 8}
  8 : {7 u i 9}
  9 : {8 i o 0}
  - : {0 p [ =}
  \end{verbatim}
\end{multicols}


\pagebreak
\section{Appendix 4: Detailed Search Engine Plot}
\begin{figure}[H]
  \centerline{\includegraphics[width=\textwidth]{eng_vs_acc_9_way}}
  \caption{Accuracies for each length of each search engine compared}\label{fig:9searchengines}
\end{figure}

\end{document}
