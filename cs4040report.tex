\documentclass{csfourzero}
\usepackage{url, natbib}

\title{The effect of corruption on search engine query prediction}
\author{Charlie F.\ Egan}
\date{\today}

\bibliographystyle{plain}
\abstract{Search engines have are able to learn from user behavoir to improve their results. Rather than relying on a traditional spelling correction algorithms, user query patterns can be used to train the system and provide more accurate predictions. This report investigates how increasingly corrupted input effects the prediction accuracy of the system and well as how each in a number of top search engines compare in their ability to predict intended input. Related work and concepts are also discussed. It was found that search engines differ significantly in their prediction accuracy and that there is no relation between the word length performance.}

\begin{document}
\maketitle

\section{Introduction}
\label{sec:intro}

Search engines offering query predictions are the research interest of the project. Search engines offer predictions to overcome corruption in user queries, where corruption is the combined effect of one or more typographical errors. While prediction performance is better than that of traditional spelling correction algorithms SOURCE the detailed processes behind predictions are not publicized. On a fundamental level, failed user queries followed by the user's own corrections are used to build a list of corrupted inputs for a desired result SOURCE.

The project aim is to evaluate the variation in the prediction accuracy in responses, for increasingly corrupted terms, queried on a number of common search engines. Prediction accuracy is capability of a search engine suggest or adopt the intended term, given a corrupted version. The effect of query length in characters, for a given rate of corruption, is also investigated as part of the project.

Information about search engine prediction accuracy for corrupted queries would be useful for those making mechanical use of search engines for spelling correction SOURCE. A ranked list of search engines by prediction accuracy could also be of interest to those with poor spelling or dexterity from conditions such as Dyslexia or Parkinson's disease. Also of interest would be the relationship between daily users and the performance of query prediction as a feature.

Sections 2-7 cover: spelling correction background as a task in natural language processing, the research questions, the design of the comparison experiments, the results, their discussion and conclusion respectively.

\section{Background and Related Work}
\label{sec:lit}

A review of related work \cite{p2pbookv2,Burnett,p2pwiki}

Guide length: 500 words.

Further explain terms and problem and reading you have done - even if not papers, give a focus for my paper, "others have looked into...", only need 2-3 relevant papers that are well discussed.

Computerized spelling correction has been an active area of research since YEAR SOURCE. Early approaches, as well as more simplistic implementations today such as those found browsers, offer predications for strings with typographical errors by calculating \textit{edit} or \textit{Levenshtein } distance. The edit distance of two strings is the number of \textit{edit operations} required to transform one string to another SOURCE 200 PAGE. In 1966 Levenshtein described a model for such transformations SOURCE 1996. \textit{Levenshtein distance} has been the basis for many such implementations since (Error Model for Noisy Channel Spelling - section 2 SOURCE if required).

Not only is calculating edit distances for an string against terms large dictionaries is computationally expensive (SOURCE An adaptive spell checker based on ps3m) these implementations are limited by the words as described by the dictionaries. There is no account taken of the relative key positions or that (SOME OTHER THING ABOUT TYPOS) SOURCE. For example, \textit{Facebokk} is more likely than \textit{Facebozk} to be a corrupted query for \textit{Facebook} though they have the same Edit and Levenshtein distances. Also it is not possible to make phonetic comparisons between strings, such as between \textit{'an introduction to \textbf{sea} programming'} and the expected result.


\section{Research question}
\label{sec:rq}

Here is clearly much work being done in the space of spelling correction automation, implementations used by search engine companies are closed source and little is known of the specifics CHECK. One aim of this project is to investigate these systems to see what can be inferred about their implementation and whether or not they differ. The research questions for the project are as follows:

\begin{itemize}
  \item{What is the effect of query length on the suggestion accuracy for a given rate of corruption?}
  \item{Is there a significant difference in prediction accuracy between search engines?}
\end{itemize}

To address these questions the prediction accuracy for a number of search engines will be compared when queried with corrupted terms of various lengths. Each search engine will be tested with the same set of corrupted terms and the accuracy of their returned predictions recorded.

The following search engines will be tested: \textit{Ask, Baidu, Bing, DuckDuckGo, Google, Sogou, Yahoo, Yandex and Youdao}. These non-aggregating search engines all offer suggestions for misspelled queries. A parallel scraper GITHUB has been implemented to collect the results of each for a given query. Five instances of this scraper will be deployed to disposable environments on the Heroku PaaS and requests made against each to gather results from a local script.

The terms selected to derive corruptions will be .com website names of a given length selected from the Alexa Top 500 ALEXA, these seed terms are listed in appendix BLAH. Brand names that are either four, eight or twelve characters in length were selected to allow a constant corruption rate of 25\% to be applied. Longer word seeds were few and were excluded to ensure predominantly single word brands names were used.

Corruptions will be generated from seed terms using a substitution algorithm, see appendix BLAH. A corruption is applied by selecting a random, uncorrupted character and substituting it with an adjacent key on the keyboard to simulate typographical errors. Transformations, as based on the US (UK Macintosh) keyboard, are listed in appendix BLAH. Insertions of additional letters, letter deletions and transpositions will not be included to ensure consistency.

In related work, spelling correction system performance is commonly assessed using Accuracy, alongside both Precision and Recall. In this experiment where, all queries are of single class, corrupted at 25\%, search engines will be compared on their \textit{Accuracy}. That is, for a given query set, what percentage of of corrupted queries cause the search engine to return the seed term. Given their rarity, collisions of corrupted spellings with other uncorrupted queries are assumed to be insignificant.

\section{Experimental Design}
\label{sec:exp}

detail test data and give justification for decisions

The null hypotheses for the experiment are as follows:
\begin{itemize}
  \item{At a constant rate of corruption, accuracy is not significantly affected by the query length in characters. (H1)}
  \item{Search engines do not differ significantly in their average accuracy for a range of query lengths. (H2)}
\end{itemize}

The target population is all search terms that are the result of one or more typographical corruptions. Due to search engine rate-limiting and time constraints it is infeasible to test these in large numbers. Random seed terms such as strings of words would not be representative of real world queries. A consistent collection of realistic query phrases is challenging to generate without introducing bias. In this experiment Alexa rank word or compound word terms 4, 8 and 12 characters in length were selected. These are highly representative of typical queries and can be consistently corrupted at 25\%, such restrictions help reduce any bias. There were too few 16 character terms for them to be included as an additional set of seed terms.

At each query length (4,8,12), 100 terms were generated from a number of seed terms with 25\% corruption ALG APP. Seed terms used for each length can be found in appendix BLAH. Accuracy is defined as the percentage of seed terms returned for their corresponding corrupted term. The search engine accuracy and term length are the independent and dependent variables respectively.

To test the hypotheses, the search engine responses for the each of the 100 corrupted terms for each length will be recorded. The accuracies of each search engine at each word length will be compared. Should the differences in average accuracy be found to be significant (p < 0.05) between length then there will be reason to reject the null hypothesis \textit{H1}.

If the search engine accuracy difference is found to be significant (p < 0.05) then this will be cause to reject \textbf{H2}.


\section{Results}
\label{sec:results}

Present the results. A good way to organise this is via subsections
for each hypothesis you tested. Include graphs of results
, tests of significance, etc. If you have
negative results, include them. A negative results is just as
informative and useful as a positive one, sometimes more so.

Guide length: 500 words.

table of the results in summary, same sentence ref a graph of the table.

the results suggest that all decreased in accuracy as the number of corruptions increased. Also x did better then y - quote values to support / highlight things in the table

then do a section for each comparison and hypothesis / quesion 5.1 5.2 - say was was and was not significant, in response to the question. Quote those p values!

Segment the section based on the number of questions, accuracy vs speed etc. for each quote the values of the statistical tests.

quote the p values and the difference in the results from the tukey HSD

\section{Discussion}
\label{sec:discuss}

What do the results say? What have you learned from the
experiments? Have you identified a correlation between variables, or
causation? What are the limitations of what you've done? What further
experiments might be of benefit?

Guide length: 400 words.

results summary, state the relationships at a high level, make references to the values of the tests mean stdev etc.

limitations of the study. limited in data and tools compared. talk about any bias, are my typos real for instance? likely not.

How to do it better next time, make better representation of the population, big pg on what was missed and how this might be an issue.

Keep it general - what does the data tell us about search engines in general?

\section{Conclusion}
\label{sec:conc}

What have you done and why? What have you shown through your
experiments?

Guide length: 100 words.

tiny, 1 pg on the work done, why done "id the best" / "test the difference", results summary, best worst wrt hyps,

I have conducted an experiment to test...

What are the implications of the results? What do they mean at a higher level?

\bibliography{myrefs}

\end{document}
